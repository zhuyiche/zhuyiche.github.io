---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am an Researcher at Midea Group, AI Lab. My research interests primarily lie in the areas of model compression, multi-modal, and robotics.

I obtained my bachelor of science from the University of Toronto in 2020, majored in Statistics. 



# üî• News
- *2024.02*: &nbsp;üéâüéâ Two papers about MLLM + Robotics have been accepted in ICRA 2024
- *2024.01*: We release a efficient multi-modal models called LLaVA-Phi, building upon Phi-2 and LLaVA.
- *2024.01*: &nbsp;üéâüéâ Two papers accepted in AAAI 2024
- *2023.12*: &nbsp;üéâüéâ One paper accepted in ICASSP 2024
- *2023.08*: &nbsp;üéâüéâ One paper accepted in ECML/PKDD 2024

# üìù Selected Publications 
&dagger; Equal Contributions, &Dagger; Corresponding Author

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2024</div><img src='images/icra_rsft.png' alt="sym" width="300" height="150"></div></div>
<div class='paper-box-text' markdown="1">

[Language-Conditioned Robotic Manipulation with Fast and Slow Thinking, **ICRA 2024**](https://arxiv.org/pdf/2401.04181.pdf)

Minjie Zhu&dagger;, **Yichen Zhu**&dagger;, Jinming Li, Junjie Wen, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, Jian Tang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2024</div><img src='images/icra_oci.png' alt="sym" width="300" height="150"></div></div>
<div class='paper-box-text' markdown="1">

[Object-Centric Instruction Augmentation for Robotic Manipulation, **ICRA 2024**](https://arxiv.org/pdf/2401.04181.pdf)

Junjie Wen&dagger;, **Yichen Zhu**&dagger;, Minjie Zhu, Jinming Li, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, Jian Tang


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='images/aaai_gail.png' alt="sym" width="300" height="150"></div></div>
<div class='paper-box-text' markdown="1">


[Exploring Gradient Explosion in Generative Adversarial Imitation Learning: A Probabilistic Perspective, **AAAI 2024**](https://arxiv.org/pdf/2312.11214)

Wanying Wang&dagger;, **Yichen Zhu**&dagger;, Yirui Zhou, Chaomin Shen, Jian Tang, Zhiyuan Xu, Yaxin Peng, Yangchun Zhang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='images/500x300.png' alt="sym" width="300" height="150"></div></div>
<div class='paper-box-text' markdown="1">

[Early Pruning with Self-Distillation for Efficient Model Compression, **AAAI 2024**]

Chen Dong&dagger;, Ning Liu&dagger;, **Yichen Zhu**, Fachao Zhang, Dong Liu, Feifei Feng, Jian Tang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2024</div><img src='images/icassp.png' alt="sym" width="300" height="150"></div></div>
<div class='paper-box-text' markdown="1">

[When Training-Free NAS meets Vision Transformer: A Neural Tangent Kernel Perspective, **ICASSP 2024**]

Qiqi Zhou, **Yichen Zhu**&Dagger;

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECML/PKDD 2023</div><img src='images/ecml.png' alt="sym" width="300" height="150"></div></div>
<div class='paper-box-text' markdown="1">

[Make a Long Image Short: Adaptive Token Length for Vision Transformers, **ECMl/PKDD 2023**](https://arxiv.org/pdf/2307.02092.pdf)

Qiqi Zhou, **Yichen Zhu**&Dagger;

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/scalekd.png' alt="sym" width="300" height="150"></div></div>
<div class='paper-box-text' markdown="1">

[ScaleKD: Distilling Scale-Aware Knowledge in Small Object Detector, **CVPR 2023**](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_ScaleKD_Distilling_Scale-Aware_Knowledge_in_Small_Object_Detector_CVPR_2023_paper.pdf)

**Yichen Zhu**, Qiqi Zhou, Ning Liu, Zhiyuan Xu, Zhicai Ou, Xiaofeng Mou, Jian Tang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='images/tllm.png' alt="sym" width="300" height="150"></div></div>
<div class='paper-box-text' markdown="1">

[Teach Less, Learn More: On the Undistillable Classes in Knowledge Distillation, **NeurIPS 2023, Spotlight (top 2%)**](https://proceedings.neurips.cc/paper_files/paper/2022/file/cf5c369c1bc070361477008e3f5210ed-Paper-Conference.pdf)

**Yichen Zhu**, Ning Liu, Zhiyuan Xu, Xin Liu, Weibin Meng, Louis Wang, Zhicai Ou, Jian Tang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2022</div><img src='images/lg3d.png' alt="sym" width="300" height="150"></div></div>
<div class='paper-box-text' markdown="1">

[Label-guided auxiliary training improves 3d object detector, **ECCV 2022**](https://arxiv.org/pdf/2207.11753)

Yaomin Huang&dagger;, Xinmei Liu&dagger;, **Yichen Zhu**, Zhiyuan Xu, Chaomin Shen, Zhengping Che, Guixu Zhang, Yaxin Peng, Feifei Feng, Jian Tang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2021</div><img src='images/sckd.png' alt="sym" width="300" height="150"></div></div>
<div class='paper-box-text' markdown="1">

[Student customized knowledge distillation: Bridging the gap between student and teacher, **ICCV 2021, Oral (top 3%)**](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Student_Customized_Knowledge_Distillation_Bridging_the_Gap_Between_Student_and_ICCV_2021_paper.pdf)

**Yichen Zhu**, Yi Wang

<!-- [**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->
<!-- - Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
<!-- </div> -->
<!-- </div> -->

<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

<!-- # üéñ Honors and Awards -->
<!-- - *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
<!-- - *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<!-- # üìñ Educations -->
<!-- - *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
<!-- - *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<!-- # üí¨ Invited Talks -->
<!-- - *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
<!-- - *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

<!-- # üíª Internships -->
<!-- - *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->